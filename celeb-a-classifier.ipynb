{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nfrom torch import nn\nimport math\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nfrom torchvision import transforms\n\ndevice = 'cpu'\nhidden_dim=64\nim_chan=3\nbatch_size=128\nmax_epochs=100000\ndisplay_step=5\nn_epochs_stop=5\nimage_size=64\ntransform = transforms.Compose([\n        transforms.Resize(image_size),\n        transforms.CenterCrop(image_size),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ])\nlr=0.01 #learning rate\ndf = pd.read_csv('/kaggle/input/celeba-dataset/list_attr_celeba.csv')\ndf = df.sample(frac=1).reset_index(drop=True)\nn_rows = df.shape[0]\nn_classes=df.shape[1]-1\n#dividir em treinamento validacao e teste\ntrain = df.iloc[:n_rows//2,:]\nval = df.iloc[n_rows//2:3*n_rows//4,:]\ntest = df.iloc[3*n_rows//4:,:]\ntrain_ids = [train.iloc[i*batch_size:min(i*batch_size+batch_size,train.shape[0]),0] for i in range(int(math.ceil(train.shape[0]/batch_size)))]\nbatches = [torch.Tensor(train.iloc[i*batch_size:min(i*batch_size+batch_size,train.shape[0]),1:].clip(0,1).values).long().to(device) for i in range(int(math.ceil(train.shape[0]/batch_size)))]\n#Estou calculando a validacao por batch para economizar memoria\n#como o conjunto de validacao tem metade do tamanho do de treinamento o tamanho do batch ou o numero de batches\n#tem que ser dividido por 2 escolhi reduzir o tamanho do batch em vez da quantidade porque assim eu economizo tempo\n#(uso o mesmo for)\nval_ids = [val.iloc[i*batch_size//2:min(i*batch_size//2+batch_size//2,val.shape[0]),0] for i in range(int(math.ceil(train.shape[0]/batch_size)))]\nval_batches = [torch.Tensor(val.iloc[i*batch_size//2:min(i*batch_size//2+batch_size//2,val.shape[0]),1:].clip(0,1).values).long().to(device) for i in range(int(math.ceil(train.shape[0]/batch_size)))]","metadata":{"_uuid":"46ede093-bd5e-421b-97f7-83120ecf8e0b","_cell_guid":"1112553f-8e32-44c8-8107-5871240f9613","collapsed":false,"execution":{"iopub.status.busy":"2021-06-05T16:12:19.075172Z","iopub.execute_input":"2021-06-05T16:12:19.075494Z","iopub.status.idle":"2021-06-05T16:12:23.092799Z","shell.execute_reply.started":"2021-06-05T16:12:19.075462Z","shell.execute_reply":"2021-06-05T16:12:23.09177Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"","metadata":{"_uuid":"708a4ccb-c3a1-4b80-80e4-2d5b12fbe278","_cell_guid":"6c389135-54f2-4572-b582-bdd9b3e02fd4","trusted":true}},{"cell_type":"code","source":"","metadata":{"_uuid":"b61e8632-cac6-4068-abaa-c4b609282a8a","_cell_guid":"779bca0b-e93d-4e55-9a0f-e6ce30a42136","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass Classif(nn.Module):\n    def __init__(self, im_chan=3, hidden_dim=64,n_classes=40):\n        super(Classif, self).__init__()\n        self.classif = nn.Sequential(\n            self.make_classif_block(im_chan, hidden_dim),\n            self.make_classif_block(hidden_dim, hidden_dim * 2),\n            self.make_classif_block(hidden_dim * 2, hidden_dim * 4, stride=3),\n            self.make_classif_block(hidden_dim * 4, n_classes, final_layer=True),\n        )\n\n    def make_classif_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n        '''\n        Function to return a sequence of operations corresponding to a block of the classifier\n        Parameters:\n            input_channels: how many channels the input feature representation has\n            output_channels: how many channels the output feature representation should have\n            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n            stride: the stride of the convolution\n            final_layer: a boolean, true if it is the final layer and false otherwise \n                      (affects activation and batchnorm)\n        '''\n        if not final_layer:\n            return nn.Sequential(\n                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n                nn.BatchNorm2d(output_channels),\n                nn.LeakyReLU(0.2, inplace=True),\n            )\n        else:\n            return nn.Sequential(\n                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n                nn.Tanh()\n            )\n\n    def forward(self, image):\n        '''\n        Function for completing a forward pass of the classifier\n        Parameters:\n            image: a flattened image tensor with dimension (im_chan)\n        '''\n        classif_pred = self.classif(image)\n        return classif_pred.view(len(classif_pred), -1)","metadata":{"_uuid":"86248eb3-7166-42e9-8814-1680ad3b301e","_cell_guid":"ad1181b3-ad1b-40ec-8d73-fd8245ec6b0c","collapsed":false,"execution":{"iopub.status.busy":"2021-06-05T16:12:27.2878Z","iopub.execute_input":"2021-06-05T16:12:27.288194Z","iopub.status.idle":"2021-06-05T16:12:27.298088Z","shell.execute_reply.started":"2021-06-05T16:12:27.28816Z","shell.execute_reply":"2021-06-05T16:12:27.29648Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classif_loss = nn.MSELoss()","metadata":{"_uuid":"4f0c81b2-8694-4585-81e5-eefd2d9c192d","_cell_guid":"7d40af2e-6487-48c4-93fa-e4ca2a2af814","collapsed":false,"execution":{"iopub.status.busy":"2021-06-05T16:12:28.009315Z","iopub.execute_input":"2021-06-05T16:12:28.009631Z","iopub.status.idle":"2021-06-05T16:12:28.013565Z","shell.execute_reply.started":"2021-06-05T16:12:28.009601Z","shell.execute_reply":"2021-06-05T16:12:28.012666Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#inicializar e treinar\nclassif = Classif(im_chan,hidden_dim,n_classes).to(device)\nclassif_opt = torch.optim.Adam(classif.parameters(), lr=lr)\n\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n    if isinstance(m, nn.BatchNorm2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n        torch.nn.init.constant_(m.bias, 0)\nclassif = classif.apply(weights_init)","metadata":{"_uuid":"634840e9-c243-42e6-9d35-68750d22b622","_cell_guid":"f2e28325-06df-4818-b576-85d81f6df69d","collapsed":false,"execution":{"iopub.status.busy":"2021-06-05T16:12:29.564579Z","iopub.execute_input":"2021-06-05T16:12:29.564892Z","iopub.status.idle":"2021-06-05T16:12:29.616083Z","shell.execute_reply.started":"2021-06-05T16:12:29.564862Z","shell.execute_reply":"2021-06-05T16:12:29.615187Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#treinamento\nmin_val_loss=float('inf')\ncur_step = 0\nepochs_no_improve=0\ntraining_losses = []\nvalidation_losses = []\nfor epoch in range(max_epochs):\n    mean_train_loss=0\n    mean_val_loss=0\n    for batch_index in tqdm(range(len(batches))):\n        batch = batches[batch_index]\n        val_batch = val_batches[batch_index]\n        train_image_list = [transform(Image.open('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'+im_id)) for im_id in train_ids[batch_index]]\n        images = torch.stack(train_image_list).to(device)\n        classif_opt.zero_grad()\n        training_pred = classif(images).float()\n        training_loss = classif_loss(training_pred,batch.float())\n        training_loss.backward()\n\n        # Update the weights\n        classif_opt.step()\n        \n        #Estou calculando a validacao por batch para economizar memoria\n        val_image_list = [transform(Image.open('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'+im_id)) for im_id in val_ids[batch_index]]\n        val_images = torch.stack(val_image_list).to(device)\n        val_pred = classif(val_images).float()\n        val_loss = classif_loss(val_pred,val_batch)\n        \n        # Keep track of the average losses\n        mean_train_loss += training_loss.item()/len(batches)\n        mean_val_loss += val_loss.item()/len(batches)\n    training_losses += [mean_train_loss]\n    validation_losses += [mean_val_loss]\n    ### Visualization code ###\n    if cur_step % display_step == 0 and cur_step > 0:\n        training_mean = sum(training_losses[-display_step:]) / display_step\n        #step_bins = 20\n        num_examples = (len(training_losses) )#// step_bins) * step_bins\n        plt.plot(\n            range(num_examples),# // step_bins), \n            torch.Tensor(training_losses[:num_examples]),\n            label=\"Training Loss\"\n        )\n        plt.plot(\n            range(num_examples),#// step_bins), \n            torch.Tensor(validation_losses[:num_examples]),\n            label=\"Validation Loss\"\n        )\n        plt.legend()\n        plt.show()\n    if(mean_val_loss < min_val_loss):\n        epochs_no_improve=0\n        min_val_loss = mean_val_loss\n    else:\n        epochs_no_improve+=1\n        if(epochs_no_improve>n_epochs_stop):\n            print('Early stopping!' )\n            break;\n    cur_step += 1\nclassif.eval()","metadata":{"_uuid":"c5052299-d20e-4aa3-a9ff-ef267d52e6f0","_cell_guid":"79fc3ad3-debb-4919-ba60-66ba60866dc9","collapsed":false,"execution":{"iopub.status.busy":"2021-06-05T16:12:30.913832Z","iopub.execute_input":"2021-06-05T16:12:30.91456Z","iopub.status.idle":"2021-06-05T19:13:37.652401Z","shell.execute_reply.started":"2021-06-05T16:12:30.914517Z","shell.execute_reply":"2021-06-05T19:13:37.651093Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_test_loss = 0\nfor i in range(int(math.ceil(test.shape[0]/batch_size))):\n    test_images = torch.stack([transform(Image.open('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'+im_id)).to(device) for im_id in test.iloc[i*batch_size:min(i*batch_size+batch_size,train.shape[0]),0]])\n    test_pred = classif(test_images).float()\n    test_loss = classif_loss(test_pred,torch.Tensor(test.iloc[i*batch_size:min(i*batch_size+batch_size,train.shape[0]),1:].values).long().to(device))\n    mean_test_loss += test_loss.item()/int(math.ceil(test.shape[0]/batch_size))\nprint(mean_test_loss)","metadata":{"_uuid":"5cdd6730-ca42-41d7-907e-53e3a2f47db5","_cell_guid":"243efcd9-669a-4aa9-9c05-7a9916c874e8","collapsed":false,"execution":{"iopub.status.busy":"2021-06-05T19:13:37.654633Z","iopub.execute_input":"2021-06-05T19:13:37.655069Z","iopub.status.idle":"2021-06-05T19:20:33.557626Z","shell.execute_reply.started":"2021-06-05T19:13:37.655023Z","shell.execute_reply":"2021-06-05T19:20:33.556657Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(classif,'classif.pt')","metadata":{"_uuid":"0a28a777-3682-4ba0-8855-526728ca97ef","_cell_guid":"c5deb0b8-bbe0-41d6-98fd-686c19f85deb","collapsed":false,"execution":{"iopub.status.busy":"2021-06-05T19:20:33.558934Z","iopub.execute_input":"2021-06-05T19:20:33.55925Z","iopub.status.idle":"2021-06-05T19:20:33.575925Z","shell.execute_reply.started":"2021-06-05T19:20:33.559219Z","shell.execute_reply":"2021-06-05T19:20:33.574737Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"fd8d093d-45a6-4641-b22c-3332f57120bd","_cell_guid":"e1b2c6da-294a-4d87-90b1-27c7bb53dc63","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}